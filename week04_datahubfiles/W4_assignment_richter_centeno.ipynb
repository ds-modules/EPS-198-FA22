{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "# Earthquake Occurrence Statistics: Gutenberg-Richter Law\n",
    "\n",
    "Declustering seismic data.\n",
    "\n",
    "**In class we:**\n",
    "- Loaded a Bay Area seismic catalog around the Loma Prieta Quake.\n",
    "- Computed the distance and time interval between Loma Prieta quake and subsequant earthquakes to indentify aftershocks.\n",
    "- Filtered the aftershocks from the catalog and looked at their distribution.\n",
    "\n",
    "**In this homwork:**\n",
    "- Load a Bay Area seismic catalog since 1900.\n",
    "- Define a function that uses a `for` loop to identify aftershocks for all the main shock events.\n",
    "- Remove the aftershocks from the catalog (decluster).\n",
    "- Explore the distribution of main shock events.\n",
    "- Examine annual earthquake rate and recurrence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import otter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "The frequency of earthquake recurrence as a function of magnitude has been a focus of seismological research since Gutenberg and Richter's pioneering work (Gutenberg and Richter, 1949). The evidence shows that the numbers of earthquakes in a given time period scales logarthmically with magnitude. To first order there are 10 times more magnitude 5 earthquakes compared to magnitude 6 events, and 10 times more magnitude 4 earthquakes compared to magnitude 5 quakes.\n",
    "\n",
    "Gutenberg and Richter found that when the base-10 logarithm of the number of earthquakes is plotted vs. magnitude that the distribution may be plotted as the line, log10(N)=A+Bm, where N is the number of earthquakes, m is the magnitude and A and B are the slope and intercept of a line. For the example described above the B-value is equal to -1 (there are 10 times fewer earthquakes for an increase of one magnitude unit). An important point to keep in mind that these parameters are based on a primary earthquake catalog in which aftershocks have been removed. **The process of aftershock removal is called declustering.**\n",
    "\n",
    "Why is this important? The A- and B-values are often used to characterize the rates of earthquakes to use in earthquake forecasting and can be a distinguising characteristic of different classes of earthquakes. The B-value (slope parameter) is often used to distinquish between 'normal' and 'swarm-like' earthquake behavior. In geothermal areas it has been observed that the earthquake distribution is richer in small earthquakes indicating a B-value significantly less than -1. \n",
    "\n",
    "Gutenberg Richter is also used to characterize seismic hazard in a region by defining the annual rate of earthquake occurrence. In this notebook you will analyze an earthquake catalog downloaded from the Northern California Earthquake Data Center from around the Berkeley Campus. You will learn how to decluster the seismicity catalog, estimate the Gutenberg Richter A- and B- values, and estimate the annual recurrence rates of large earthquake in the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Load the Earthquake Catalog\n",
    "\n",
    "Load the .csv data file 'data/bay_area_anss_1900_2020.csv' of all the earthquakes 1900 - 2020 in the ANSS (Advanced National Seismic System) catalog from around Berkeley (latitudes 37.0-38.75° and longitude -123.35 to -121.5°; [http://ncedc.org/anss/catalog-search.html](http://ncedc.org/anss/catalog-search.html)). Convert the column DateTime to a datetime object with `pd.to_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# read data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bay_catalog\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m bay_catalog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      4\u001b[0m bay_catalog\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "bay_catalog=pd.read_csv('')\n",
    "bay_catalog['DateTime'] = \n",
    "bay_catalog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create data arrays, it will speed up our loops later\n",
    "year=bay_catalog['DateTime'].dt.year\n",
    "month=bay_catalog['DateTime'].dt.month\n",
    "day=bay_catalog['DateTime'].dt.day\n",
    "lat=bay_catalog['Latitude'].values\n",
    "lon=bay_catalog['Longitude'].values\n",
    "mag=bay_catalog['Magnitude'].values\n",
    "nevt=len(year)        #number of events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "<font color=darkred>**_Concept question:_**</font> How many total events are in this catalog before we remove the aftershocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Map the Raw Earthquake Catalog\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the raw catalog. Scale the marker color and size by magnitude. Add a colorbar with a label. \n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "lat0=37.0\n",
    "lat1=38.75\n",
    "lon0=-123.25\n",
    "lon1=-121.5\n",
    "tickstep=0.25 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "Berk_lat = 37.8716\n",
    "Berk_lon = -122.2727\n",
    "\n",
    "# make plot object with ticks, coastlines, etc.\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',title='Raw Catalog')\n",
    "\n",
    "\n",
    "# Sort by magnitude to plot largest events on top\n",
    "bay_catalog_sorted = ...\n",
    "#exponent to scale marker size\n",
    "z=...\n",
    "\n",
    "plt.scatter(..., ..., s=..., c=..., cmap='plasma',alpha=0.4,marker='o') # plot circles on EQs\n",
    "plt.plot(Berk_lon,Berk_lat,'s',color='limegreen',markersize=8)  # plot green square on Berkeley Campus\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Plot Magnitude vs. Time for the Raw Catalog\n",
    "\n",
    "Plot magnitude vs. time for the raw catalog as we did in class. Use the `alpha = 0.2` argument in `plot` to make the markers transparent. Add labels and a title.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# plot magnitude vs. time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "<font color=darkred>**_Concept questions:_**</font> The first event in the catalog is a few years after the Great 1906 San Francisco Earthquake. Read a bit about it here [The Great 1906 San Francisco Earthquake](https://earthquake.usgs.gov/earthquakes/events/1906calif/18april/). That event inspired scientists to start recording the earthquake catalog. But early instruments for measuring earthquakes were not as good as modern seismometers. When are there events missing from this catalog i.e. are there large gaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Plot Histogram of Magnitude for the Raw Catalog\n",
    "\n",
    "Plot a histogram of the magnitudes in the raw catalog, with a log y-axis. Be sure to label your axes.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# plot a histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Compute the B-value of the Gutenberg-Richter Law for the raw catalog\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# the observed log10 number of events per year as function of magnitude (data)\n",
    "min_mag=0.0\n",
    "max_mag=6.9\n",
    "m_raw=np.arange(min_mag,max_mag,0.1)\n",
    "N_raw=np.zeros(len(m_raw))\n",
    "numyr=2020-1910\n",
    "for i in range(0,len(m_raw),1):\n",
    "    N_raw[i]=np.log10(...)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Use the polyfit() function as done in class to determine the B-value.\n",
    "\n",
    "How does the value compare to what was found in class? \n",
    "\n",
    "**Write your answer here**\n",
    "\n",
    "Note that we should not use this B-value for characterizing hazard since it is biased by the large number of aftershocks in the catalog. Next we will decluster the catalog as was done in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# Solve for Model Parameters (A- and B-values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Define the haversine function to compute distance\n",
    "\n",
    "You will need to call this function to compute the distance between each event in the catalog and future earthquakes for possible identification as aftershocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Do Not Modify this Cell\n",
    "#This function computes the spherical earth distance between to geographic points and is used in the\n",
    "#declustering algorithm below\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) # convert degrees lat, lon to radians\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2  # great circle inside sqrt\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))   # great circle angular separation\n",
    "    km = 6371.0 * c   # great circle distance in km, earth radius = 6371.0 km\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Now develop code to decluster the catalog as demonstrated in class\n",
    "\n",
    "- First compute the number of days from the first event in the catalog to all subsequent earthquakes\n",
    "\n",
    "- Then apply the Gardner and Knopoff (1974) time/distance window method to identify aftershocks and remove them from the catalog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "#### This is a reminder of the time and distance equations of the Gardner and Knopoff (1974) method\n",
    "\n",
    "<img src=\"Figures/window_approx.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Write your code here to compute the 'days' array that will be used in the declustering\n",
    "#algorithm\n",
    "\n",
    "days=np.zeros(nevt) # initialize the size of the array days\n",
    "\n",
    "for i in range(0,nevt,1):\n",
    "    #code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Declustering Algorithm Time!\n",
    "\n",
    "You'll build a `for` loop for indentifying aftershocks in the seismic catalog. In-class we just found the aftershocks for one main shock event. Here you will add a `for` loop that iterates through the whole catalog and finds all events that can be cassified as aftershocks. Replace the xxx's in the code below:\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Decluster the Catalog  Note: This cell may take a few minute to complete\n",
    "cnt=0 # initialize a counting variable\n",
    "save=np.zeros(10000000) # initialize a counting variable\n",
    "for xxx   # step through EQ catalog\n",
    "    # logical if statements to incorporate definitions of Dtest and Ttest aftershock window bounds\n",
    "    Dtest=xxx   # distance bounds\n",
    "    if mag[i] >= 6.5:\n",
    "        Ttest=xxx  # aftershock time bounds for M >= 6.5\n",
    "    else:\n",
    "        Ttest=xxx  # aftershock time bounds for M < 6.5\n",
    "    \n",
    "    a=days[...]-days[i]    # time interval in days to subsequent earthquakes in catalog\n",
    "    m=mag[i+1:nevt]   # magnitudes of subsequent earthquakes in catalog\n",
    "    b=haversine_np(lon[i],lat[i],lon[i+1:nevt],lat[i+1:nevt]) # distance in km to subsequent EQs in catalog\n",
    "    \n",
    "    icnt=np.count_nonzero(a <= Ttest)   # counts the number of potential aftershocks, \n",
    "                                        # the number of intervals <= Ttest bound\n",
    "    if icnt > 0:  # if there are potential aftershocks\n",
    "        itime=np.array(np.nonzero(a <= Ttest)).transpose() + (i+1) # indices of potential aftershocks <= Ttest bound\n",
    "        for j in range(0,icnt,1):   # loops over the aftershocks         \n",
    "            if b[j] <= Dtest and m[j] < mag[i]: # test if the event is inside the distance window \n",
    "                                                # and that the event is smaller than the current main EQ\n",
    "                save[cnt]=itime[j]  # index value of the aftershock\n",
    "                cnt += 1 # increment the counting variable\n",
    "\n",
    "                \n",
    "af_ind=np.delete(np.unique(save),0)   # This is an array of indexes that will be used to delete events flagged \n",
    "                                      # as aftershocks    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "Use `np.delete(array,indices_to_delete)` to delete the aftershock events which are located at indexes `af_ind` from your original arrays. You can also use `dataframe_name.drop(indices_to_delete,axis=0)` to delete rows from a DataFrame.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# delete the aftershock events\n",
    "declustered_df = ...\n",
    "declustered_days=...  #The aftershocks are deleted from the days array \n",
    "declustered_mag=...  #The aftershocks are deleted from the mag array \n",
    "declustered_lon=...  #The aftershocks are deleted from the lon array \n",
    "declustered_lat=...  #The aftershocks are deleted from the lat array \n",
    "n=len(declustered_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "<font color=darkred>**_Concept question:_**</font> How many of the raw events catalog were main-shocks vs. after-shocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Plot Magnitude vs. Time for the Declustered catalog\n",
    "\n",
    "Plot magnitude vs. time for the raw catalog and add on top the declustered catalog with slightly larger markers, so the main-shocks in the raw catalog are covered and the aftershocks remain. Add a legend to label the two sets of markers. Add labels and a title.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# plot magnitude vs. time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept question:_**</font> At approximately what dates (or in days since first EQ) do you observe a large aftershock sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Plot Histogram of Magnitude for the Declustered Catalog\n",
    "\n",
    "Plot a histogram of the magnitudes in the raw catalog, with a log y-axis. Add a histogram of the declustered catalog on top to compare. Add a legend to label the two datasets. Add labels and a title.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# plot a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Compute the B-value of the Gutenberg-Richter Law for the declustered catalog\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "# Write code here to first determine the Gutenberg-Richter distribution (log10(N) vs magnitude_bin)\n",
    "# Then fit a line to the data to determine A- and B-values. Only fit the portion of the data\n",
    "# That is representative of a 'completeness' (i.e. the portion of the distribution that \n",
    "# behaves linearly). Do this 'filtering' before solving for the A- and B- values will give a\n",
    "# more robust result\n",
    "\n",
    "# Besure to make a plot that compares your Gutenberg-Richter line fit to the declustered catalog\n",
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "**Compare and discuss the differences between the raw and declustered data and the estimated G&R model parameters**\n",
    "\n",
    "**Write answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "## Map the Declustered Earthquake Catalog\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the declustered catalog. Scale the marker color and size by magnitude. You'll also add the locations of major faults in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Load locations of faults to add to our map\n",
    "a=pd.read_table('data/Hayward.txt') \n",
    "hay_lon=a['x'].values\n",
    "hay_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/San_Andreas.txt') \n",
    "SA_lon=a['x'].values\n",
    "SA_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/San_Gregorio.txt') \n",
    "SG_lon=a['x'].values\n",
    "SG_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Calaveras.txt') \n",
    "cal_lon=a['x'].values\n",
    "cal_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Hunting_Creek.txt') \n",
    "HC_lon=a['x'].values\n",
    "HC_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Rodgers_Creek.txt') \n",
    "RC_lon=a['x'].values\n",
    "RC_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Concord.txt') \n",
    "con_lon=a['x'].values\n",
    "con_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Greenville.txt') \n",
    "grn_lon=a['x'].values\n",
    "grn_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Maacama.txt') \n",
    "m_lon=a['x'].values\n",
    "m_lat=a['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "Plot map of main shock events with the marker size and color scaled and sorted by magnitude, and plot the fault locations on top. Add a legend.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "\n",
    "\n",
    "# make plot object with ticks, coastlines, etc.\n",
    "plt.figure(1,(10,10))\n",
    "\n",
    "\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "\n",
    "\n",
    "plt.scatter(..., ..., s=..., c=...,vmax=6.9, cmap='plasma',alpha=0.4,marker='o',label='Main Earthquakes')\n",
    "plt.plot(hay_lon,hay_lat,'-',color='red',linewidth=2,label='Hayward Fault')\n",
    "plt.plot(SA_lon,SA_lat,'-',color='orange',linewidth=2,label='San Andreas Fault')\n",
    "plt.plot(SG_lon,SG_lat,'-',color='yellow',linewidth=2,label='San Gregorio Fault')\n",
    "plt.plot(cal_lon,cal_lat,'-',color='green',linewidth=2,label='Calaveras Fault')\n",
    "plt.plot(con_lon,con_lat,'-',color='cyan',linewidth=2,label='Concord Fault')\n",
    "plt.plot(grn_lon,grn_lat,'-',color='lime',linewidth=2,label='Greenville Fault')\n",
    "plt.plot(m_lon,m_lat,'-',color='blueviolet',linewidth=2,label='Maacama Fault')\n",
    "plt.plot(RC_lon,RC_lat,'-',color='magenta',linewidth=2,label='Rodgers Creek Fault')\n",
    "plt.plot(HC_lon,HC_lat,'-',color='blue',linewidth=2,label='Hunting Creek Fault')\n",
    "plt.plot(Berk_lon,Berk_lat,'rs',markersize=8,label='UC Berkeley')  # plot red square on Berkeley Campus\n",
    "plt.colorbar(shrink=0.8,label='Magnitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "<img src=\"Figures/fault_map.png\" width=550>\n",
    "Map of Bay Area faults. \n",
    "Source: https://pubs.er.usgs.gov/publication/fs20163020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "<font color=darkred>**_Concept question:_**</font> What faults have historically been active since 1911? What mapped bay-area fault has the highest probability of rupturing with M$\\geq6.7$ in the near future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Earthquake recurrence in the San Francisco Bay Area\n",
    "\n",
    "Based on the 120 year declustered catalog estimate the average recurrence interval for magnitude 5.0, 6.0, 7.0 and 8.0 events in the San Francisco Bay Area. Note that when compiling the G&R data it should have been 'annualized' by dividing by the number of years in the catalog. Then the recurrence interval in years is simply 1/annual_rate.\n",
    "\n",
    "How reasonable do you think these recurrence intervals are, and what do they imply about the likelihood of larger earthquake occurrence in the San Francisco Bay Area (you may want to consider when previous large earthquake occurred, i.e. https://www.usgs.gov/natural-hazards/earthquake-hazards/science/back-future-san-andreas-fault?qt-science_center_objects=0#qt-science_center_objects). What kind of data helps inform on earthquake recurrence allowing us to look earlier in time than catalog statistics allows?\n",
    "\n",
    "**Write answer here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style='font-size:20px'> <br>Save this notebook, then click <a href='W4_assignment_richter_centeno.pdf' download>here</a> to open the pdf.<br></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grader = otter.Notebook()\n",
    "from otter.export import export_notebook\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"W4_assignment_richter_centeno.ipynb\", filtering=True, pagebreaks=False)\n",
    "display(HTML(\"<p style='font-size:20px'> <br>Save this notebook, then click <a href='W4_assignment_richter_centeno.pdf' download>here</a> to open the pdf.<br></p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "include"
    ]
   },
   "source": [
    "### Turn in this notebook\n",
    "\n",
    "Save your completed notebook as a pdf file and upload to bcourses.\n",
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
