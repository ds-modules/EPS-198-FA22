{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loma Prieta Earthquake, Earthquake Occurrence Statistics: Omori's Law \n",
    "\n",
    "**Last week we:**\n",
    "- pandas DataFrames, indexing, and data cleaning.\n",
    "- Load marine geophysical data (bathymetry and marine magnetic anomalies) from two oceanic ridges.\n",
    "- Select data and drop rows with NaNs.\n",
    "- Plot bathymetry data and evaluate spreading rate.\n",
    "- Declare a functions to detrend data, calculate distance and estimate spreading rate\n",
    "- Plot marine magnetic anomaly data and compare spreading rates.\n",
    "\n",
    "**Our goals for today:**\n",
    "- Load a Bay Area seismic catalog of January 01,1989 to December 31, 1995.\n",
    "- Compute the distance and time interval between Loma Prieta quake and subsequant earthquakes to indentify aftershocks.\n",
    "- Filter the aftershocks from the catalog and look at their distribution.\n",
    "- Examine the Gutenberg-Richter statistic\n",
    "- Make and earthquake forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On October 17 at 5:15pm PDT (October 18 1989 at 04:15am UTC) the M6.9 Loma Prieta earthquake occurred in the Santa Cruz mountains approximately 80 km southwest of the Berkeley Campus. We will use this earthquake sequence to investigate the performance of catalog declustering algorithm.\n",
    "\n",
    "https://en.wikipedia.org/wiki/1989_Loma_Prieta_earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Earthquake Catalog\n",
    "\n",
    "Load the .csv data file of all the earthquakes from January 01,1989 to December 31, 1995 in the ANSS (Advanced National Seismic System) catalog from between latitudes 36.0-38.5° and longitude -123.0 to -121.0° ([http://ncedc.org/anss/catalog-search.html](http://ncedc.org/anss/catalog-search.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "#pd.to_datetime?\n",
    "LP_catalog=pd.read_csv('data/bay_area_anss_1989_1995.csv')\n",
    "#the following function just reformats to a DateTime object format\n",
    "LP_catalog['DateTime'] = pd.to_datetime(LP_catalog['DateTime'])\n",
    "LP_catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DateTime objects are very useful. There are functions to parse the DateTime data into individual arrays, or functions to process the DateTime data in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets explore some features of datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create data arrays, it will speed up our loops later\n",
    "#  Note .dt provides functional resources to work on the DateTime file. \n",
    "year=LP_catalog['DateTime'].dt.year\n",
    "month=LP_catalog['DateTime'].dt.month\n",
    "day=LP_catalog['DateTime'].dt.day\n",
    "lat=LP_catalog['Latitude'].values\n",
    "lon=LP_catalog['Longitude'].values\n",
    "mag=LP_catalog['Magnitude'].values\n",
    "nevt=len(year)        #number of events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the Raw Earthquake Catalog\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the raw catalog. Scale the marker color and size by magnitude.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "lat0=36.0\n",
    "lat1=38.5\n",
    "lon0=-123.0\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "Berk_lat = 37.8716\n",
    "Berk_lon = -122.2727\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks)\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',title='Raw Catalog')\n",
    "\n",
    "# Sort by magnitude to plot largest events on top\n",
    "LP_catalog_sorted = LP_catalog.sort_values(...)\n",
    "#exponent to scale marker size\n",
    "z=np.exp(...)    \n",
    "\n",
    "plt.scatter(..., ..., s=..., \n",
    "            c=LP_catalog_sorted['Magnitude'], cmap='plasma',alpha=0.4,marker='o') # plot circles on EQs\n",
    "plt.plot(Berk_lon,Berk_lat,'s',color='limegreen',markersize=8)  # plot red square on Berkeley Campus\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Magnitude vs. Time for the Raw Catalog\n",
    "\n",
    "Plot magnitude vs. time for the raw catalog and print out the number of events as we did in-class. Use the `alpha = 0.2` argument in `plot` to make the markers transparent.\n",
    "\n",
    "We have an array of magnitudes but what about time?  We need to construct an array of fractions of years from the first entry in the catalog.\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a time array and then use the plot() function to plot the earthquakes\n",
    "#as points.\n",
    "\n",
    "d0 = datetime.date(..., ..., ...)\n",
    "time=np.zeros(nevt) # initialize the size of the array days\n",
    "for i in range(0,nevt,1):\n",
    "    d1 = datetime.date(..., ..., ...)\n",
    "    time[i]=((d1 - d0).days)/365.25 + year[0]\n",
    "\n",
    "\n",
    "#Now the plot\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(..., ...,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='Time', ylabel='Magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is another way to do it using some built in functions. It is as clear but it is \n",
    "#powerful using the datetime attributes, and you may want to use it when comparing other datetime ranges.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.plot(mdates.date2num(LP_catalog['DateTime']), mag,'o',alpha=0.2,markersize=5)\n",
    "locator = mdates.AutoDateLocator()\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "ax.set(xlabel='Time', ylabel='Magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthquake Catalog Statistics and Aftershocks\n",
    "\n",
    "There are two primary statistics that describe earthquake catalog data. The first is Omori's Law which describes the distribution of aftershocks following a primary earthquake or mainshock. It can be used to estimate the rate of aftershock production over time after the earthquake.\n",
    "\n",
    "The second is Gutenberg-Richter, which is used to characterize the rates of primary earthquakes over some period of time to determine rates of occurence, which can then be used to assess hazard and make earthquake forecasts.\n",
    "\n",
    "Both statistics ***require a means of identifying earthquakes as aftershocks***. Since aftershocks are related to the redistribution of mainshock stress including them in a statistic that assumes `primary (or mainshock)` event occurrence would lead to biased results. Aftershocks are therefore identified and removed from analysis. This is done based on spatial and temporal proximity of a given earthquake to another earlier, larger earthquake. There are several empirical models developed for this identification and the following cell describes one of these.\n",
    "\n",
    "### We will start by examing the Loma Prieta aftershocks, which we need to isolate from the catalog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gardner and Knopoff (1974) Aftershock Model\n",
    "\n",
    "Gardner and Knopoff (1974) studied many hundreds of earthquake sequences and empirically determined distance and time filters as a function of mainshock magnitude to help identify aftershocks. The table below shows their model with distance to the event in km, and time after the event in days. \n",
    "\n",
    "Both distance from the mainshock and time afterward increase dramatically with increasing magnitude. Note that aftershocks following M5.5+ earthquakes can last for years.\n",
    "\n",
    "<img src=\"Figures/aftershock_windows.png\" width=600>\n",
    "\n",
    "These windows are based on the following equations:\n",
    "<img src=\"Figures/window_approx.png\" >\n",
    "\n",
    "To build our algorithm to identify aftershock using these windows we need to convert the year-month-day formate of dates to a timeline in number of days. We'll do this using the function `datetime.date` which for a given year, month, and day returns a datetime class object, which can be used to compute the time interval in days.\n",
    "\n",
    "We will also use the use the `np.power(base, exponent)` function to compute the distance and time interval bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  First, Determine the index value and the number of days from the beginning of the catalog to the Loma Prieta Quake\n",
    "\n",
    "The Loma Prieta earthquake is the largest in the catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP_ind = LP_catalog.index[...]\n",
    "lp_mag=LP_catalog['Magnitude'][LP_ind].values\n",
    "print(f'Loma Prieta Earthquake M{lp_mag[0]:.1f} Index={LP_ind[0]:d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some information about using datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To calculate the days since the Loma Prieta earthquake for many entries we can use a loop\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the number of days from the Loma Prieta Earthquake\n",
    "days=np.zeros(nevt) # initialize the size of the array days\n",
    "\n",
    "for i in range(0,nevt,1):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some of the days[...] values negative?\n",
    "\n",
    "***Write your answer here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, Define a function to compute the distance between to geographic points on a sphere - we will use the haversine function from last week\n",
    "\n",
    "<img src=\"Figures/great_circle_eqn.png\" >\n",
    "\n",
    "\n",
    "<img src=\"Figures/Illustration_of_great-circle_distance.svg\" >\n",
    "Great-circle distance shown in red between two points on a sphere, P and Q. \n",
    "Source: https://en.wikipedia.org/wiki/Great-circle_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function computes the spherical earth distance between to geographic points and is used in the\n",
    "#declustering algorithm below\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) # convert degrees lat, lon to radians\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2  # great circle inside sqrt\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))   # great circle angular separation\n",
    "    km = 6371.0 * c   # great circle distance in km, earth radius = 6371.0 km\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we need to define the code for applying the Gardner & Knopoff (1974) Aftershock Model\n",
    "<img src=\"Figures/window_approx.png\" >\n",
    "\n",
    "<font color=goldenrod>**_Code for you to write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "\n",
    "print(f'Magnitude={mag[i]:.1f}  Dtest={Dtest:0.2f} km  Ttest={Ttest:.2f} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the pieces together to build a aftershock detection algorithm\n",
    "\n",
    "<font color=goldenrod>**_Code for us to examine and write_**</font>\n",
    "\n",
    "Note that you need to be careful about array dimensionality (column vs row ordered). The dimensionalty when performing certain (most) operations on arrays needs to be the same for both. We will also use np.array() which has as default column major ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Declustering Algorithm Catalog\n",
    "cnt=LP_ind[0] # initialize a counting variable at the LP index\n",
    "save=np.zeros((1000000),dtype=int) # initialize a storage array\n",
    "\n",
    "i=LP_ind[0]\n",
    "# logical if statements to incorporate definitions of Dtest and Ttest aftershock window bounds\n",
    "\n",
    "\n",
    "a=days[i+1:nevt]-days[i]    # time interval in days to subsequent earthquakes in catalog\n",
    "m=mag[i+1:nevt]   # magnitudes of subsequent earthquakes in catalog\n",
    "b=haversine_np(lon[i],lat[i],lon[i+1:nevt],lat[i+1:nevt]) # distance in km to subsequent EQs in catalog\n",
    "\n",
    "#First Test find number of events that are within the aftershock time period\n",
    "icnt=np.count_nonzero(a <= Ttest)   # counts the number of potential aftershocks\n",
    "    \n",
    "if icnt > 0:  # if there are potential aftershocks\n",
    "    itime=np.array(np.nonzero(a <= Ttest)).transpose() + (i+1) # indices of potential aftershocks <= Ttest bound\n",
    "    for j in range(0,icnt,1):   # loops over the aftershocks         \n",
    "        if b[j] <= Dtest and m[j] < mag[i]: # test if the event is inside the distance window \n",
    "                                            # and that the event is smaller than the current main EQ\n",
    "            save[cnt]=itime[j]  # index value of the aftershock\n",
    "            cnt += 1 # increment the counting variable\n",
    "\n",
    "                \n",
    "af_ind=np.delete(np.unique(save),0)   # This is an array of indexes of aftershocks    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of arrays for the aftershock catalog. Use `af_ind` to select the aftershock events from the raw calalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the aftershock events\n",
    "aftershock_df = LP_catalog.iloc[af_ind] \n",
    "aftershock_days=days[af_ind]  #The aftershocks are selected from the days array \n",
    "aftershock_mag=mag[af_ind]    #The aftershocks are selected from the mag array \n",
    "aftershock_lon=lon[af_ind]    #The aftershocks are selected from the lon array \n",
    "aftershock_lat=lat[af_ind]    #The aftershocks are selected from the lat array \n",
    "n2=len(aftershock_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Concept question:_**</font> How many aftershock events were there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Magnitude vs. Time for the Aftershock Catalog\n",
    "\n",
    "Plot magnitude vs. time for the aftershock events and print out the number of events. Use the `alpha = 0.2` argument in `plot` to make the markers transparent. Plot the mainshock in a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot DeClustered Catalog\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.plot(mdates.date2num(LP_catalog.iloc[LP_ind]['DateTime']), mag[LP_ind[0]],'o',color='red',markersize=10,label='Loma Prieta')\n",
    "ax.plot(mdates.date2num(aftershock_df['DateTime']), aftershock_mag,'o',alpha=0.2,markersize=5,label='Aftershocks')\n",
    "locator = mdates.AutoDateLocator()\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='Magnitude',\n",
    "       title='Aftershock Event Catalog')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a histogram of `aftershock_days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(...)\n",
    "plt.xlabel('Days after main shock')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.title('Aftershocks of the Loma Prieta 1989 Quake')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Discussion question:_**</font> How would you describe the distribution of number of aftershocks with time after the main quake?\n",
    "\n",
    "By how much is the rate of aftershock occurrence reduced after 7 days? After 30 days?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the Aftershock Events\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the aftershock catalog. Again scale the marker color and size by magnitude, set `vmax=6.9` so the colorscale matches our original map. Add the locations of UC Berkeley campus and the Loma Prieta event epicenter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of Aftershock events\n",
    "\n",
    "#Set Corners of Map\n",
    "lat0=36.0\n",
    "lat1=38.5\n",
    "lon0=-123.0\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.set_aspect('auto')\n",
    "ax.coastlines(resolution='10m',linewidth=1) #downloaded 10m, 50m\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',\n",
    "       title='Aftershock Catalog')\n",
    "\n",
    "\n",
    "#Sort Descending to plot largest events on top\n",
    "indx=np.argsort(aftershock_mag)   #determine sort index\n",
    "x=aftershock_lon[indx]            #apply sort index\n",
    "y=aftershock_lat[indx]\n",
    "z=np.exp(aftershock_mag[indx])    #exponent to scale size\n",
    "\n",
    "plt.scatter(x, y, s=z, c=aftershock_mag[indx],vmax=6.9, cmap='plasma',alpha=0.4,marker='o',label='aftershocks')\n",
    "plt.plot(lon[LP_ind[0]], lat[LP_ind[0]],'*',color='yellow',markersize=20,markeredgecolor='black',label='Loma Prieta Quake')\n",
    "plt.plot(-122.2727,37.8716,'rs',markersize=8,label='UC Berkeley')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/fault_map.png\" width=700>\n",
    "Map of Bay Area faults. \n",
    "Source: https://pubs.er.usgs.gov/publication/fs20163020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>**_Discussion questions:_**</font>  \n",
    "- Were aftershocks only triggered on the same fault as the main quake?\n",
    "- What faults were active? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets decluster the whole catalog and examine the Gutenberg-Richter Statistic\n",
    "\n",
    "Gutenberg devised a statistic that counts, for a given time period, the number of events with magnitude equal to or greater than a given magnitude. It is computed by defining a range of test magnitudes (sometimes called bins) and then determining the number of events of that magnitude and larger. Therefore it is a statistic on the rate of earthquake above a given magnitude. When plotted in log10(number(mag)) vs. mag there is a linear trend with an approximate -1 slope. That means that for each unit increase in magnitude there is a 10-fold decrease in the number of earthquakes. Gutenberg and Richter fit a line to the data compiled in this way to determine the intercept and slope (b-value, or rate of earthquake occurrence. \n",
    "\n",
    "$log10(N(m)) = A + B*M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Algorithm to Decluster the entire catalog\n",
    "#Reset variables\n",
    "year=np.array(LP_catalog['DateTime'].dt.year)\n",
    "month=np.array(LP_catalog['DateTime'].dt.month)\n",
    "day=np.array(LP_catalog['DateTime'].dt.day)\n",
    "lat=LP_catalog['Latitude'].values\n",
    "lon=LP_catalog['Longitude'].values\n",
    "mag=LP_catalog['Magnitude'].values\n",
    "nevt=len(year)        #number of events \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Loma Prieta earthquake we only investigated the catalog from the time of the event to identify the aftershocks to plot the Omori curve.\n",
    "\n",
    "More typically the declustering algorithm is used to remove the aftershocks from a catalog. To apply the declustering algorithm to the entire catalog we need to pass through the entire catalog from beginning to end. To do this we need to add a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to add a loop over time from what we had before\n",
    "\n",
    "cnt=0\n",
    "save=np.zeros(10000000,dtype=int) # initialize a counting variable\n",
    "\n",
    "for i in range(nevt):                        #You only need this outside loop with proper indentation\n",
    "    Dtest=np.power(10,0.1238*mag[i]+0.983)   # distance bounds\n",
    "    if mag[i] >= 6.5:\n",
    "        Ttest=np.power(10,0.032*mag[i]+2.7389)  # aftershock time bounds for M >= 6.5\n",
    "    else:\n",
    "        Ttest=np.power(10,0.5409*mag[i]-0.547)  # aftershock time bounds for M < 6.5\n",
    "    \n",
    "    a=days[i+1:nevt]-days[i]    # time interval in days to subsequent earthquakes in catalog\n",
    "    m=mag[i+1:nevt]   # magnitudes of subsequent earthquakes in catalog\n",
    "    b=haversine_np(lon[i],lat[i],lon[i+1:nevt],lat[i+1:nevt]) # distance in km to subsequent EQs in catalog\n",
    "    \n",
    "    icnt=np.count_nonzero(a <= Ttest)   # counts the number of potential aftershocks, \n",
    "                                    # the number of intervals <= Ttest bound\n",
    "    \n",
    "    if icnt > 0:  # if there are potential aftershocks\n",
    "        itime=np.array(np.nonzero(a <= Ttest)).transpose() + (i+1) # indices of potential aftershocks <= Ttest bound\n",
    "    \n",
    "    for j in range(0,icnt,1):   # loops over the aftershocks         \n",
    "        if b[j] <= Dtest and m[j] < mag[i]: # test if the event is inside the distance window \n",
    "                                            # and that the event is smaller than the current main EQ\n",
    "            save[cnt]=itime[j]  # index value of the aftershock\n",
    "            cnt += 1 # increment the counting variable\n",
    "            \n",
    "af_ind=np.delete(np.unique(save),0)   # This is an array of indexes of aftershocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The np.delete() function is used to remove aftershocks from the lat, lon, etc arrays. It is good to rename the receiving array (plat, plot, etc - p for 'primary')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This time Delete the aftershocks from the catalog and create new arrays\n",
    "plat=np.delete(lat,af_ind)\n",
    "plon=np.delete(lon,af_ind)\n",
    "pmag=np.delete(mag,af_ind)\n",
    "pyear=np.delete(year,af_ind)\n",
    "pmonth=np.delete(month,af_ind)\n",
    "pday=np.delete(day,af_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many events were removed from the catalog? What percentage of the events are considered primary, or mainshocks?\n",
    "\n",
    "***Write your answers here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RePlot Mag vs Time and compare with what was obtained before\n",
    "\n",
    "d0 = datetime.date(pyear[0], 1, 1)\n",
    "time=np.zeros(len(pday)) # initialize the size of the array days\n",
    "for i in range(0,len(pday),1):\n",
    "    d1 = datetime.date(pyear[i], pmonth[i], pday[i])\n",
    "    time[i]=((d1 - d0).days)/365.25 + year[0]\n",
    "\n",
    "\n",
    "#This loop created a time array in units of years.\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(time, pmag,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='Time', ylabel='Magnitude',\n",
    "       title='Raw Event Catalog')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map a map of the declustered earthquake catalog and compare with the complete catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "lat0=36.0\n",
    "lat1=38.5\n",
    "lon0=-123.0\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "Berk_lat = 37.8716\n",
    "Berk_lon = -122.2727\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks)\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',title='Raw Catalog')\n",
    "\n",
    "# Sort by magnitude to plot largest events on top\n",
    "#LP_catalog_sorted = LP_catalog.sort_values(by=['Magnitude'])\n",
    "#exponent to scale marker size\n",
    "z=np.exp(pmag)    \n",
    "\n",
    "plt.scatter(plon, plat, s=z, \n",
    "            c=pmag, cmap='plasma',alpha=0.4,marker='o') # plot circles on EQs\n",
    "plt.plot(Berk_lon,Berk_lat,'s',color='limegreen',markersize=8)  # plot red square on Berkeley Campus\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Gutenberg Richter Statistic\n",
    "\n",
    "- Compile the annual number of earthquakes above a given magnitude\n",
    "- Examine the distribution\n",
    "- Fit the Gutenberg Richter model to the data\n",
    "- Use the Gutenberg Richter model to forecast recurrence of earthquakes\n",
    "\n",
    "$log10(N(m)) = A + B*M$\n",
    "\n",
    "Let's try it.\n",
    "\n",
    "<font color=goldenrod>**_Code for us to examine and write_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute G&R Statistics\n",
    "min_mag=0.0\n",
    "max_mag=max(pmag)\n",
    "\n",
    "m=np.arange(min_mag,max_mag,0.1)    #The magnitude \"bins\"\n",
    "N=np.zeros(len(m))                  #N is the number of events\n",
    "numyr=1995-1989 + 1                 #number of years in the catalog - note 7 years is too low for this analysis\n",
    "\n",
    "for i in range(0,len(m),1):\n",
    "    N[i]=np.log10(...)\n",
    " \n",
    "#plot it\n",
    "plt.figure()\n",
    "plt.plot(m,N,'o')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('log10(N)')\n",
    "plt.title('Gutenberg Ricter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Fit the G&R model to the data and plot\n",
    "\n",
    "y=np.polyfit(...)\n",
    "intercept=y[1]\n",
    "slope=y[0]\n",
    "\n",
    "print(f'The declustered data B value: {slope:0.3f}')\n",
    "\n",
    "#plot it\n",
    "plt.figure()\n",
    "plt.plot(m,N,'o')\n",
    "plt.plot(m,intercept+slope*m,'-',color='orange')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('log10(N)')\n",
    "plt.title('Gutenberg Ricter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the annual rate of M4, M5 and M6+ earthquakes? What is the average recurrence interval?\n",
    "\n",
    "Note that this result is significantly biased because here we used only a 6 year catalog. In the takehome assignment you will explore a more expansive catalog.\n",
    "\n",
    "***write answer here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn in this notebook\n",
    "\n",
    "Save your completed notebook to a pdf file and upload to bcourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
